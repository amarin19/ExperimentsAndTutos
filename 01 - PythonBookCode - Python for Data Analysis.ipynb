{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Analysis\n",
    "##### Reproduced by Albert Marin\n",
    "\n",
    "_Chapters 1_ to _3_ in this book are just for reviewing general stuff about the python environment. Therefore, we can skip all of those and jump into action :)!\n",
    "\n",
    "Firstly, we should import all the libraries that are going to be needed in this notebook. Note that we are going to configure the plots to be in line with the code as well as the style of the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "import sys\n",
    "\n",
    "#Inline plots:\n",
    "%matplotlib inline\n",
    "\n",
    "#Beautiful R-like plots:\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: NumPy review\n",
    "Now let's make a fast review of the numpy arrays with a dummy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random matrix: \n",
      "[[ 0.28356098  0.76727679  0.72284837]\n",
      " [ 0.66196884  0.21997557  0.77913749]]\n",
      "\n",
      "Shape of the matrix (rows, columns): (2, 3)\n",
      "Dimension of the matrix: 2\n",
      "\n",
      "Change the type of matrix: \n",
      "[[ 0.28356099+0.j  0.76727676+0.j  0.72284836+0.j]\n",
      " [ 0.66196883+0.j  0.21997556+0.j  0.77913749+0.j]]\n",
      "\n",
      "Operations ********************************************************************************\n",
      "1. Addition: \n",
      "[[ 0.56712196  1.53455359  1.44569674]\n",
      " [ 1.32393769  0.43995113  1.55827498]]\n",
      "\n",
      "2. Multiplication by scalar: \n",
      "[[ 2.83560982  7.67276793  7.22848369]\n",
      " [ 6.61968844  2.19975566  7.7913749 ]]\n",
      "\n",
      "3. Multiplication bite-wise: \n",
      "[[ 0.08040683  0.58871368  0.52250976]\n",
      " [ 0.43820275  0.04838925  0.60705523]]\n",
      "\n",
      "4. Multiplication (AxB): \n",
      "[[ 1.19163027  0.91968895]\n",
      " [ 0.91968895  1.09364723]]\n",
      "\n",
      "Slicing ***********************************************************************************\n",
      "1. First row: [ 0.28356098  0.76727679  0.72284837]\n",
      "\n",
      "2. First element of second row: 0.6619688435236506\n",
      "\n",
      "3. Last two elements of first row: [ 0.76727679  0.72284837]\n",
      "\n",
      "4. Elements in the whole matrix that are over 0.5: [ 0.76727679  0.72284837  0.66196884  0.77913749]\n",
      "\n",
      "5. Elements of the first row that are over 0.5: [ 0.76727679  0.72284837]\n",
      "\n",
      "6. Elements of the first row that are over 0.5 and less than 0.7: []\n",
      "\n",
      "7. The same, with a mask: []\n",
      "\n",
      "Others ************************************************************************************\n",
      "1. Now the first column is -9: \n",
      "[[-9.          0.76727679  0.72284837]\n",
      " [-9.          0.21997557  0.77913749]]\n",
      "\n",
      "2. Now everything less than 0.6 is 0: \n",
      "[[ 0.          0.76727679  0.72284837]\n",
      " [ 0.          0.          0.77913749]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = np.random.random(size = (2,3))\n",
    "\n",
    "#Properties and display:\n",
    "print('A random matrix: \\n{}\\n'.format(data))\n",
    "print('Shape of the matrix (rows, columns): {}\\nDimension of the matrix: {}\\n'.format(data.shape, data.ndim))\n",
    "print('Change the type of matrix: \\n{}\\n'.format(data.astype(np.complex64)))\n",
    "\n",
    "#Operations:\n",
    "print('Operations {}'.format(80*'*'))\n",
    "print('1. Addition: \\n{}\\n'.format(data+data))\n",
    "print('2. Multiplication by scalar: \\n{}\\n'.format(data*10))\n",
    "print('3. Multiplication bite-wise: \\n{}\\n'.format(data*data))\n",
    "print('4. Multiplication (AxB): \\n{}\\n'.format(np.dot(data,data.T)))\n",
    "\n",
    "#Slicing:\n",
    "print('Slicing {}'.format(83*'*'))\n",
    "print('1. First row: {}\\n'.format(data[0]))\n",
    "print('2. First element of second row: {}\\n'.format(data[1][0]))\n",
    "print('3. Last two elements of first row: {}\\n'.format(data[0][-2:]))\n",
    "print('4. Elements in the whole matrix that are over 0.5: {}\\n'.format(data[data>0.5]))\n",
    "print('5. Elements of the first row that are over 0.5: {}\\n'.format(data[0,data[0]>0.5]))\n",
    "print('6. Elements of the first row that are over 0.5 and less than 0.7: {}\\n'.format(\n",
    "        data[0][(data[0] > 0.5) & (data[0] < 0.7)]))\n",
    "mask = (data[0] > 0.5) & (data[0] < 0.7)\n",
    "print('7. The same, with a mask: {}\\n'.format(data[0][mask]))\n",
    "\n",
    "#Renaming values:\n",
    "print('Others {}'.format(84*'*'))\n",
    "data[:,0] = -9\n",
    "print('1. Now the first column is -9: \\n{}\\n'.format(data))\n",
    "data[data<0.6] = 0\n",
    "print('2. Now everything less than 0.6 is 0: \\n{}\\n'.format(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other very useful functions to create ndarrays are: **`arange`, `ones`, `zeros`, `empty`, `eye`**.\n",
    "For element-wise operations, they are the same as one could expect: `max`, `abs`, `sqrt`, etc.\n",
    "\n",
    "A finding function that deserves its own place iw the `np.where()` because it's super fast. It's usually used to make new arrays based on a condition.  Suppose you had a matrix of data and you wanted to replace all positive values with 2 and all negative values with -2 or only the positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix: \n",
      "[[-2.45117979  0.00854989  0.6879897   0.89944026]\n",
      " [-0.61285632  1.20429941 -1.55071196 -0.13599763]\n",
      " [-0.247793    2.0573997   1.04829997 -1.49928059]\n",
      " [-1.29392961 -1.38517953 -1.05323869  1.92243222]]\n",
      "\n",
      "Replace both positive and negative values: \n",
      "[[-2  2  2  2]\n",
      " [-2  2 -2 -2]\n",
      " [-2  2  2 -2]\n",
      " [-2 -2 -2  2]]\n",
      "\n",
      "Replace only positive values: \n",
      "[[-2.45117979  2.          2.          2.        ]\n",
      " [-0.61285632  2.         -1.55071196 -0.13599763]\n",
      " [-0.247793    2.          2.         -1.49928059]\n",
      " [-1.29392961 -1.38517953 -1.05323869  2.        ]]\n"
     ]
    }
   ],
   "source": [
    "data2 = np.random.rand(4,4)*np.random.choice([-2.5,2.5],(4,4))\n",
    "print('Original matrix: \\n{}\\n'.format(data2))\n",
    "data3 = np.where(data2 > 0, 2, -2)\n",
    "print('Replace both positive and negative values: \\n{}\\n'.format(data3))\n",
    "data4 = np.where(data2 > 0, 2, data2)\n",
    "print('Replace only positive values: \\n{}'.format(data4))\n",
    "#Delete all the variables\n",
    "del data, mask, data2, data3, data4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: basic `pandas` in action\n",
    "Pandas has a structure that has to be learn, as every module or package. In order to do so, let's create dummy data structures and work with them to exemplify the cases to handle the pandas structure.\n",
    "\n",
    "### 5.1 pd.Series( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "0    2\n",
      "1    3\n",
      "2   -9\n",
      "3    7\n",
      "dtype: int64\n",
      "\n",
      "Values: [ 2  3 -9  7]\n",
      "\n",
      "Indexes: RangeIndex(start=0, stop=4, step=1)\n",
      "\n",
      "Renamed indexes (I):\n",
      "a    2\n",
      "b    3\n",
      "c   -9\n",
      "d    7\n",
      "dtype: int64\n",
      "\n",
      "Renamed indexes (II):\n",
      "a    2\n",
      "b    3\n",
      "c   -9\n",
      "d    7\n",
      "dtype: int64\n",
      "\n",
      "Renamed attributes:\n",
      "Alphabet\n",
      "a    2\n",
      "b    3\n",
      "c   -9\n",
      "d    7\n",
      "Name: Dummy thing :), dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  BASIC CRETAION OF SERIES:\n",
    "data = pd.Series([2,3,-9,7])\n",
    "\n",
    "#Information of the series:\n",
    "print('Data:\\n{}\\n'.format(data))\n",
    "print('Values: {}\\n'.format(data.values))\n",
    "print('Indexes: {}\\n'.format(data.index))\n",
    "\n",
    "#Redefine the series with names for the indexes:\n",
    "data.index = ['a','b','c','d']\n",
    "print('Renamed indexes (I):\\n{}\\n'.format(data))\n",
    "data = pd.Series([2,3,-9,7], index = ['a','b','c','d'])\n",
    "print('Renamed indexes (II):\\n{}\\n'.format(data))\n",
    "data.name = 'Dummy thing :)'\n",
    "data.index.name = 'Alphabet'\n",
    "print('Renamed attributes:\\n{}\\n'.format(data))\n",
    "\n",
    "#Delete the names we created because it's annoying for the examples:\n",
    "data.name, data.index.name = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    2\n",
      "b    3\n",
      "c   -9\n",
      "d    7\n",
      "dtype: int64\n",
      "\n",
      "1. Access the data with key \"a\": 2\n",
      "\n",
      "2. Access the data with keys \"d\", \"b\", \"c\" in this particular order:\n",
      "d    7\n",
      "b    3\n",
      "c   -9\n",
      "dtype: int64\n",
      "\n",
      "3. Access the edited data with key \"a\": 0\n",
      "\n",
      "4. Check data that is greather than 0:\n",
      "b    3\n",
      "d    7\n",
      "dtype: int64\n",
      "\n",
      "5. Multiply data by 5:\n",
      "a     0\n",
      "b    15\n",
      "c   -45\n",
      "d    35\n",
      "dtype: int64\n",
      "\n",
      "6. Check existence of \"c\" in data: True\n"
     ]
    }
   ],
   "source": [
    "#  ACCESSING DATA AND CHANGING IT:\n",
    "print(data, end = '\\n\\n')\n",
    "print('1. Access the data with key \"a\": {}\\n'.format(data['a']))\n",
    "print('2. Access the data with keys \"d\", \"b\", \"c\" in this particular order:\\n{}\\n'.format(data[['d', 'b', 'c']]))\n",
    "\n",
    "data['a'] = 0\n",
    "print('3. Access the edited data with key \"a\": {}\\n'.format(data['a']))\n",
    "print('4. Check data that is greather than 0:\\n{}\\n'.format(data[data > 0]))\n",
    "print('5. Multiply data by 5:\\n{}\\n'.format(data*5))\n",
    "print('6. Check existence of \"c\" in data: {}'.format('c' in data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Panda objects can be built from lists, numpy arrays (as seen) or dictionaries. When using a dictionary, the keys will be used as indexes sorted by alphabetical order. Also, an interesting thing is that when we are selecting a subset of the panda's object, if the value doesn't exist, it will store it as a `nan` value as shown below: the vowels different from `a` are not defined in the series so they will be undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    0.0\n",
      "e    NaN\n",
      "b    3.0\n",
      "i    NaN\n",
      "c   -9.0\n",
      "o    NaN\n",
      "d    7.0\n",
      "u    NaN\n",
      "dtype: float64\n",
      "\n",
      "We can check if the data is missing:\n",
      "a    False\n",
      "e     True\n",
      "i     True\n",
      "o     True\n",
      "u     True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "data2 = data[['a', 'e', 'b', 'i', 'c', 'o', 'd', 'u']]\n",
    "print('{}\\n'.format(data2))\n",
    "print('We can check if the data is missing:\\n{}'.format(data2[['a', 'e', 'i', 'o', 'u']].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, if we are trying to add something that doesn't exist in two different series, the result will be `nan` for the values that are present in only one series.\n",
    "\n",
    "### 5.2 pd.DataFrame( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by default:\n",
      "        City  Pop  Year\n",
      "0  Barcelona  1.5  2015\n",
      "1  Barcelona  1.7  2016\n",
      "2  Barcelona  3.6  2017\n",
      "3     Lleida  2.4  2016\n",
      "4     Lleida  2.9  2017\n",
      "\n",
      "Sorted as we want:\n",
      "   Year       City  Pop\n",
      "0  2015  Barcelona  1.5\n",
      "1  2016  Barcelona  1.7\n",
      "2  2017  Barcelona  3.6\n",
      "3  2016     Lleida  2.4\n",
      "4  2017     Lleida  2.9\n",
      "\n",
      "Original:\n",
      "           c    b     a\n",
      "e  Barcelona  1.5  2015\n",
      "d  Barcelona  1.7  2016\n",
      "c  Barcelona  3.6  2017\n",
      "b     Lleida  2.4  2016\n",
      "a     Lleida  2.9  2017\n",
      "Sort by index:\n",
      "           c    b     a\n",
      "a     Lleida  2.9  2017\n",
      "b     Lleida  2.4  2016\n",
      "c  Barcelona  3.6  2017\n",
      "d  Barcelona  1.7  2016\n",
      "e  Barcelona  1.5  2015\n",
      "Sort by columns:\n",
      "      a    b          c\n",
      "e  2015  1.5  Barcelona\n",
      "d  2016  1.7  Barcelona\n",
      "c  2017  3.6  Barcelona\n",
      "b  2016  2.4     Lleida\n",
      "a  2017  2.9     Lleida\n",
      "Sort by population:\n",
      "        City  Pop  Year\n",
      "0  Barcelona  1.5  2015\n",
      "1  Barcelona  1.7  2016\n",
      "3     Lleida  2.4  2016\n",
      "4     Lleida  2.9  2017\n",
      "2  Barcelona  3.6  2017\n",
      "\n",
      "Missing data works like in pd.Series:\n",
      "   Year       City  Pop  PIB\n",
      "a  2015  Barcelona  1.5  NaN\n",
      "b  2016  Barcelona  1.7  NaN\n",
      "c  2017  Barcelona  3.6  NaN\n",
      "d  2016     Lleida  2.4  NaN\n",
      "e  2017     Lleida  2.9  NaN\n",
      "\n",
      "             Year       Pop\n",
      "count     5.00000  5.000000\n",
      "mean   2016.20000  2.420000\n",
      "std       0.83666  0.864292\n",
      "min    2015.00000  1.500000\n",
      "25%    2016.00000  1.700000\n",
      "50%    2016.00000  2.400000\n",
      "75%    2017.00000  2.900000\n",
      "max    2017.00000  3.600000\n"
     ]
    }
   ],
   "source": [
    "#  BASIC CRETAION OF SERIES:\n",
    "data = {'City': ['Barcelona', 'Barcelona', 'Barcelona', 'Lleida', 'Lleida'],\n",
    "        'Year': [2015, 2016, 2017, 2016, 2017],\n",
    "        'Pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\n",
    "frame = pd.DataFrame(data)\n",
    "\n",
    "#It is automatically sorted, but we can change it:\n",
    "print('Sorted by default:\\n{}\\n'.format(frame))\n",
    "print('Sorted as we want:\\n{}\\n'.format(pd.DataFrame(data, columns=['Year', 'City', 'Pop'])))\n",
    "\n",
    "frame_letters = pd.DataFrame(frame.values, index=list('edcba'), columns=list('cba'))\n",
    "print('Original:\\n{}\\nSort by index:\\n{}\\nSort by columns:\\n{}\\nSort by population:\\n{}\\n'.format(\n",
    "    frame_letters, frame_letters.sort_index(),\n",
    "    frame_letters.sort_index(axis = 1), frame.sort_values(by = 'Pop')))\n",
    "# <!> Adding the \"ascending\" parameter, we can sort it in inverse order <!>\n",
    "\n",
    "frame2 = pd.DataFrame(data, columns=['Year', 'City', 'Pop', 'PIB'], index = list('abcde'))\n",
    "print('Missing data works like in pd.Series:\\n{}\\n'.format(frame2))\n",
    "\n",
    "#Describe the numerical values:\n",
    "print(frame2.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other functions similar to `describe()` that can be used are basic statistics such as `min()`, `var()`, `kurt()`, etc. Also, if we want to obtain a fancy output, it can be achieved by just calling the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>City</th>\n",
       "      <th>Pop</th>\n",
       "      <th>PIB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>2015</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>2016</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>1.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>2017</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>3.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>2016</td>\n",
       "      <td>Lleida</td>\n",
       "      <td>2.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>2017</td>\n",
       "      <td>Lleida</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year       City  Pop  PIB\n",
       "a  2015  Barcelona  1.5  NaN\n",
       "b  2016  Barcelona  1.7  NaN\n",
       "c  2017  Barcelona  3.6  NaN\n",
       "d  2016     Lleida  2.4  NaN\n",
       "e  2017     Lleida  2.9  NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing data in a dataframe has different points:\n",
    " * A column in a DataFrame can be retrieved as a Series either by dict-like notation or by attribute.\n",
    " * A row can also be retrieved by position or name by a couple of methods, such as the iloc indexing field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See which ones we have: Index(['Year', 'City', 'Pop', 'PIB'], dtype='object')\n",
      "\n",
      "Check the cities:\n",
      "a    Barcelona\n",
      "b    Barcelona\n",
      "c    Barcelona\n",
      "d       Lleida\n",
      "e       Lleida\n",
      "Name: City, dtype: object\n",
      "\n",
      "Check the population:\n",
      "a    1.5\n",
      "b    1.7\n",
      "c    3.6\n",
      "d    2.4\n",
      "e    2.9\n",
      "Name: Pop, dtype: float64\n",
      "\n",
      "Check the row \"b\" by using its name:\n",
      "Year         2016\n",
      "City    Barcelona\n",
      "Pop           1.7\n",
      "PIB           NaN\n",
      "Name: b, dtype: object\n",
      "\n",
      "Check the row \"b\" by using its index in the indexes list:\n",
      "Year         2016\n",
      "City    Barcelona\n",
      "Pop           1.7\n",
      "PIB           NaN\n",
      "Name: b, dtype: object\n",
      "\n",
      "Check the year from the row \"b\": 2016\n",
      "\n",
      "Check the values for the years later than 2015:\n",
      "   Year       City  Pop  PIB\n",
      "b  2016  Barcelona  1.7  NaN\n",
      "c  2017  Barcelona  3.6  NaN\n",
      "d  2016     Lleida  2.4  NaN\n",
      "e  2017     Lleida  2.9  NaN\n"
     ]
    }
   ],
   "source": [
    "#Columns:\n",
    "print('See which ones we have: {}\\n'.format(frame2.columns))\n",
    "print('Check the cities:\\n{}\\n'.format(frame2.City))\n",
    "print('Check the population:\\n{}\\n'.format(frame2['Pop']))\n",
    "\n",
    "#Rows:\n",
    "print('Check the row \"b\" by using its name:\\n{}\\n'.format(frame2.loc['b']))\n",
    "print('Check the row \"b\" by using its index in the indexes list:\\n{}\\n'.format(frame2.iloc[1]))\n",
    "\n",
    "#Access a value:\n",
    "print('Check the year from the row \"b\": {}\\n'.format(frame2.iloc[1].Year))\n",
    "\n",
    "#Boolean indexing:\n",
    "print('Check the values for the years later than 2015:\\n{}'.format(frame2[frame2.Year > 2015]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to modify a value, we can do it exactly the same as the Series. The only difference here is that if we want to change the whole column, we only need to specify one single value for the whole column. If we assign less values than it needs, they will be full of `nan`. Consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year       City  Pop  PIB\n",
      "a  2015  Barcelona  1.5  0.9\n",
      "b  2016  Barcelona  1.7  0.9\n",
      "c  2017  Barcelona  3.6  0.9\n",
      "d  2016     Lleida  2.4  0.9\n",
      "e  2017     Lleida  2.9  0.9\n",
      "\n",
      "   Year       City  Pop   PIB\n",
      "a  2015  Barcelona  1.5   NaN\n",
      "b  2016  Barcelona  1.7  0.95\n",
      "c  2017  Barcelona  3.6  2.00\n",
      "d  2016     Lleida  2.4   NaN\n",
      "e  2017     Lleida  2.9  1.20\n"
     ]
    }
   ],
   "source": [
    "frame2.PIB = 0.9\n",
    "print(frame2, end = '\\n\\n')\n",
    "\n",
    "PIBs = pd.Series([0.95, 2, 1.2], index=['b', 'c', 'e'])\n",
    "frame2.PIB = PIBs\n",
    "print(frame2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very interesting function to use is the `unique()` method for the dataFrame as it will return the set of values that are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lleida Barcelona\n",
      "\n",
      "Count how many Barcelonas and Lleidas we have:\n",
      "Barcelona    3\n",
      "Lleida       2\n",
      "Name: City, dtype: int64\n",
      "\n",
      "Original:\n",
      "   Qu1  Qu2  Qu3\n",
      "0    5    1    1\n",
      "1    2    4    1\n",
      "2    3    1    3\n",
      "3    5    5    5\n",
      "4    4    4    1\n",
      "Counted values:\n",
      "   Qu1  Qu2  Qu3\n",
      "1  0.0  2.0  3.0\n",
      "2  1.0  0.0  0.0\n",
      "3  1.0  0.0  1.0\n",
      "4  1.0  2.0  0.0\n",
      "5  2.0  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "#The * is used to unwrap the list.\n",
    "#The sorted reverse = True is used to reverse the order of the output.\n",
    "\n",
    "print(*sorted(frame2.City.unique().tolist(), reverse=True))\n",
    "\n",
    "print('\\nCount how many Barcelonas and Lleidas we have:\\n{}\\n'.format(frame2.City.value_counts()))\n",
    "\n",
    "#If we wanted to count the data in a matrix, we can use the `apply()` mapping function:\n",
    "example = pd.DataFrame({'Qu1': np.random.randint(1, high=6, size = 5), \n",
    "                  'Qu2': np.random.randint(1, high=6, size = 5),\n",
    "                  'Qu3': np.random.randint(1, high=6, size = 5)})\n",
    "print('Original:\\n{}\\nCounted values:\\n{}'.format(example, example.apply(pd.value_counts).fillna(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important feature is the `reindex()` function with the filling options of `fill_value=`, `ffill` and `bfill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fills with NaN:\n",
      "     Year       City  Pop\n",
      "0  2015.0  Barcelona  1.5\n",
      "1  2016.0  Barcelona  1.7\n",
      "2  2017.0  Barcelona  3.6\n",
      "3  2016.0     Lleida  2.4\n",
      "4  2017.0     Lleida  2.9\n",
      "5     NaN        NaN  NaN\n",
      "\n",
      "Fills with specic value:\n",
      "   Year       City  Pop\n",
      "0  2015  Barcelona  1.5\n",
      "1  2016  Barcelona  1.7\n",
      "2  2017  Barcelona  3.6\n",
      "3  2016     Lleida  2.4\n",
      "4  2017     Lleida  2.9\n",
      "5     0          0  0.0\n",
      "\n",
      "Make column and row:\n",
      "   Year       City  Pop  Record\n",
      "0  2015  Barcelona  1.5     NaN\n",
      "1  2016  Barcelona  1.7     NaN\n",
      "2  2017  Barcelona  3.6     NaN\n",
      "3  2016     Lleida  2.4     NaN\n",
      "4  2017     Lleida  2.9     NaN\n",
      "5  2017     Lleida  2.9     NaN\n"
     ]
    }
   ],
   "source": [
    "frame2 = pd.DataFrame(frame, columns = ['Year', 'City', 'Pop'])\n",
    "\n",
    "print('Fills with NaN:\\n{}\\n'.format(frame2.reindex(np.arange(6))))\n",
    "print('Fills with specic value:\\n{}\\n'.format(frame2.reindex(np.arange(6), fill_value=0)))\n",
    "print('Make column and row:\\n{}'.format(frame2.reindex([0,1,2,3,4,5], method='ffill').reindex(\n",
    "                                                    columns=['Year', 'City', 'Pop', 'Record'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An interesting operation is when we have to sum things and there are `nan` values. In these cases, we can use the option to skip or not those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df:\n",
      "   one  two\n",
      "a  1.0  NaN\n",
      "b  5.0 -2.0\n",
      "c  NaN  NaN\n",
      "d  3.0 -1.0\n",
      "\n",
      "Sum everything even it has nans:\n",
      "a    1.0\n",
      "b    3.0\n",
      "c    NaN\n",
      "d    2.0\n",
      "dtype: float64\n",
      "\n",
      "Sum if the row does not have nans:\n",
      "a    NaN\n",
      "b    3.0\n",
      "c    NaN\n",
      "d    2.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([[1, np.nan], [5, -2], [np.nan, np.nan], [3, -1]],\n",
    "                   index=['a', 'b', 'c', 'd'], columns=['one', 'two'])\n",
    "print('Df:\\n{}\\n'.format(df))\n",
    "print('Sum everything even it has nans:\\n{}\\n'.format(df.sum(axis=1)))\n",
    "print('Sum if the row does not have nans:\\n{}'.format(df.sum(axis=1, skipna = False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.3 Handling missing data\n",
    "As it can appear everywhere, it is interesting to have a few aces up the sleeve just in case we need to use them. The easiest thing to do is to check whether they are missing values with the method `.isnull()`. `NaN` and `None` will both be treated as missing values. After this, we can use a few strategies:\n",
    " * `.dropna()` method: it will simply eliminate the missing values and return a clearn dataframe. We can pass some parameters on `how` which are `all` (the rows with a full `NaN` value) or `axis=`.\n",
    " * `.fillna()` method: already seen. We can fill it with a value, an array, `ffill` or `bfill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "     0    1    2    3    4\n",
      "0  4.2  1.5  2.4  2.0  1.8\n",
      "1  1.0  6.5  3.0  2.0  NaN\n",
      "2  1.0  NaN  NaN  NaN  NaN\n",
      "3  NaN  NaN  NaN  NaN  NaN\n",
      "4  NaN  6.5  3.0  NaN  NaN\n",
      "\n",
      "Clean I drop rows with NaN:\n",
      "     0    1    2    3    4\n",
      "0  4.2  1.5  2.4  2.0  1.8\n",
      "\n",
      "Clean II empty lines (either direction):\n",
      "     0    1    2    3    4\n",
      "0  4.2  1.5  2.4  2.0  1.8\n",
      "1  1.0  6.5  3.0  2.0  NaN\n",
      "2  1.0  NaN  NaN  NaN  NaN\n",
      "4  NaN  6.5  3.0  NaN  NaN\n",
      "\n",
      "Clean III drop rows with NaN:\n",
      "     0    1    2    3    4\n",
      "0  4.2  1.5  2.4  2.0  1.8\n",
      "\n",
      "Clean IV drop empty rows:\n",
      "     0    1    2    3    4\n",
      "0  4.2  1.5  2.4  2.0  1.8\n",
      "1  1.0  6.5  3.0  2.0  NaN\n",
      "2  1.0  NaN  NaN  NaN  NaN\n",
      "4  NaN  6.5  3.0  NaN  NaN\n",
      "\n",
      "Clean V drop columns with NaN :\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4]\n",
      "\n",
      "Clean VI empty columns:\n",
      "     0    1    2    3    4\n",
      "0  4.2  1.5  2.4  2.0  1.8\n",
      "1  1.0  6.5  3.0  2.0  NaN\n",
      "2  1.0  NaN  NaN  NaN  NaN\n",
      "3  NaN  NaN  NaN  NaN  NaN\n",
      "4  NaN  6.5  3.0  NaN  NaN\n",
      "\n",
      "Clean VII thresholded rows:\n",
      "     0    1    2    3    4\n",
      "0  4.2  1.5  2.4  2.0  1.8\n",
      "1  1.0  6.5  3.0  2.0  NaN\n",
      "\n",
      "Clean VIII thresholded columns:\n",
      "     0    1    2\n",
      "0  4.2  1.5  2.4\n",
      "1  1.0  6.5  3.0\n",
      "2  1.0  NaN  NaN\n",
      "3  NaN  NaN  NaN\n",
      "4  NaN  6.5  3.0\n"
     ]
    }
   ],
   "source": [
    "#Clean by dropping:\n",
    "missing = pd.DataFrame([[4.2, 1.5, 2.4, 2., 1.8],\n",
    "                        [1., 6.5, 3., 2, np.nan],\n",
    "                        [1., np.nan, np.nan, np.nan, np.nan], \n",
    "                        [np.nan, np.nan, np.nan, np.nan, np.nan], \n",
    "                        [np.nan, 6.5, 3., np.nan, np.nan]])\n",
    "\n",
    "print('Original:\\n{}\\n'.format(missing))\n",
    "print('Clean I drop rows with NaN:\\n{}\\n'.format(missing.dropna()))\n",
    "print('Clean II empty lines (either direction):\\n{}\\n'.format(missing.dropna(how = 'all')))\n",
    "print('Clean III drop rows with NaN:\\n{}\\n'.format(missing.dropna(axis = 0)))\n",
    "print('Clean IV drop empty rows:\\n{}\\n'.format(missing.dropna(axis = 0, how = 'all')))\n",
    "print('Clean V drop columns with NaN :\\n{}\\n'.format(missing.dropna(axis = 1)))\n",
    "print('Clean VI empty columns:\\n{}\\n'.format(missing.dropna(axis = 1, how = 'all')))\n",
    "print('Clean VII thresholded rows:\\n{}\\n'.format(missing.dropna(thresh = 3)))\n",
    "print('Clean VIII thresholded columns:\\n{}'.format(missing.dropna(thresh = 3, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.4 Advanced indexing: hierarchical indexing or multiindexing\n",
    "This advanced method is used to have 2 or more index levels. It is useful to work with higher dimensional data in a lower dimensional data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a  1   -0.987740\n",
      "   2    2.316092\n",
      "   3   -1.784533\n",
      "b  1    0.627223\n",
      "   2    0.458714\n",
      "   3    0.249351\n",
      "c  1   -0.328768\n",
      "   2   -0.479698\n",
      "d  2    0.603799\n",
      "   3    1.834328\n",
      "dtype: float64\n",
      "\n",
      "MultiIndex(levels=[['a', 'b', 'c', 'd'], [1, 2, 3]],\n",
      "           labels=[[0, 0, 0, 1, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "#Simple example of multiindexed series:\n",
    "data = pd.Series(np.random.randn(10), index = [list('aaabbbccdd'),[1,2,3,1,2,3,1,2,2,3]])\n",
    "print(data, end='\\n\\n')\n",
    "print(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To access the data, is the same as in the normal indexing but adding another level. A way to re-order this is by the command `.unstack()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.987740</td>\n",
       "      <td>2.316092</td>\n",
       "      <td>-1.784533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.627223</td>\n",
       "      <td>0.458714</td>\n",
       "      <td>0.249351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>-0.328768</td>\n",
       "      <td>-0.479698</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.603799</td>\n",
       "      <td>1.834328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3\n",
       "a -0.987740  2.316092 -1.784533\n",
       "b  0.627223  0.458714  0.249351\n",
       "c -0.328768 -0.479698       NaN\n",
       "d       NaN  0.603799  1.834328"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that either axis can have the multiindexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City  Barcelona     Lleida\n",
      "Col      Orange Red Orange\n",
      "N   n                     \n",
      "One 1         0   1      2\n",
      "    2         3   4      5\n",
      "Two 1         6   7      8\n",
      "    2         9  10     11\n",
      "n\n",
      "1    0\n",
      "2    3\n",
      "Name: (Barcelona, Orange), dtype: int64\n"
     ]
    }
   ],
   "source": [
    "frame = pd.DataFrame(np.arange(12).reshape((4, 3)),index=[['One', 'One', 'Two', 'Two'], [1, 2, 1, 2]],\n",
    "                  columns=[['Barcelona', 'Barcelona', 'Lleida'], ['Orange', 'Red', 'Orange']])\n",
    "frame.index.names = ['N', 'n']\n",
    "frame.columns.names = ['City', 'Col']\n",
    "print(frame)\n",
    "print(frame['Barcelona', 'Orange']['One'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Chapter 6: data loading, storage and file formats\n",
    "Usually, we won't generate random data like we were generating in the previous chapter. We will have to load `.txt` files, or `.sql` databases and _then_ work with them. This chapter focuses in this. Remember that:\n",
    " * Statements started by `!` are meant to be executed as in the terminal.\n",
    " * Statements started by `%` are _magic_ commands.\n",
    " \n",
    "### 6.1 `read_csv( )` and `read_table( )`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a,b,c,d,message\r\n",
      "1,2,3,4,hello\r\n",
      "5,6,7,8,world\r\n",
      "9,10,11,12,foo\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>foo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a   b   c   d message\n",
       "0  1   2   3   4   hello\n",
       "1  5   6   7   8   world\n",
       "2  9  10  11  12     foo"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets peak into the csv example:\n",
    "!cat PythonForDataAnalysis/ex1.csv\n",
    "\n",
    "#Method 1:\n",
    "df = pd.read_csv('PythonForDataAnalysis/ex1.csv')\n",
    "#Method 2:\n",
    "df2 = pd.read_table('PythonForDataAnalysis/ex1.csv', sep=',')\n",
    "\n",
    "df if all(df == df2) else 'Methods are not equivalent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original headerless:\n",
      "   0   1   2   3      4\n",
      "0  1   2   3   4  hello\n",
      "1  5   6   7   8  world\n",
      "2  9  10  11  12    foo\n",
      "\n",
      "Artificially created:\n",
      "   a   b   c   d message\n",
      "0  1   2   3   4   hello\n",
      "1  5   6   7   8   world\n",
      "2  9  10  11  12     foo\n",
      "\n",
      "Using \"message\" column as indexes:\n",
      "         a   b   c   d\n",
      "message               \n",
      "hello    1   2   3   4\n",
      "world    5   6   7   8\n",
      "foo      9  10  11  12\n"
     ]
    }
   ],
   "source": [
    "#Data without headers:\n",
    "df = pd.read_csv('PythonForDataAnalysis/ex2.csv', header=None)\n",
    "print('Original headerless:\\n{}\\n'.format(df))\n",
    "print('Artificially created:\\n{}\\n'.format(pd.read_csv('PythonForDataAnalysis/ex2.csv',\n",
    "                                                     names=['a','b','c','d','message'])))\n",
    "print('Using \"message\" column as indexes:\\n{}'.format(pd.read_csv('PythonForDataAnalysis/ex2.csv',\n",
    "                                                     names=['a','b','c','d','message'],\n",
    "                                                     index_col = 'message')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key1,key2,value1,value2\r\n",
      "one,a,1,2\r\n",
      "one,b,3,4\r\n",
      "one,c,5,6\r\n",
      "one,d,7,8\r\n",
      "two,a,9,10\r\n",
      "two,b,11,12\r\n",
      "two,c,13,14\r\n",
      "two,d,15,16\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value1</th>\n",
       "      <th>value2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key1</th>\n",
       "      <th>key2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">one</th>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">two</th>\n",
       "      <th>a</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           value1  value2\n",
       "key1 key2                \n",
       "one  a          1       2\n",
       "     b          3       4\n",
       "     c          5       6\n",
       "     d          7       8\n",
       "two  a          9      10\n",
       "     b         11      12\n",
       "     c         13      14\n",
       "     d         15      16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data with multiindex:\n",
    "!cat PythonForDataAnalysis/ex3.csv\n",
    "parsed = pd.read_csv('PythonForDataAnalysis/ex3.csv', index_col = ['key1','key2'])\n",
    "parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data has a more problematic storage, we will need to use regular expressions (more info: [tutorial 1](https://docs.python.org/2/library/re.html), [tutorial 2](http://www.regular-expressions.info/)). For example, let's suppose that a file has whitespaces as delimiter but in some cases there are many of them and sometimes it's just one. We can solve it by using the `\\s+` regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A         B         C\r\n",
      "aaa -0.264438 -1.026059 -0.619500\r\n",
      "bbb 0.927272 0.302904 -0.032399\r\n",
      "ccc -0.264273 -0.386314 -0.217601\r\n",
      "ddd -0.871858 -0.348382 1.100491\r\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>-0.264438</td>\n",
       "      <td>-1.026059</td>\n",
       "      <td>-0.619500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbb</th>\n",
       "      <td>0.927272</td>\n",
       "      <td>0.302904</td>\n",
       "      <td>-0.032399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ccc</th>\n",
       "      <td>-0.264273</td>\n",
       "      <td>-0.386314</td>\n",
       "      <td>-0.217601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddd</th>\n",
       "      <td>-0.871858</td>\n",
       "      <td>-0.348382</td>\n",
       "      <td>1.100491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            A         B         C\n",
       "aaa -0.264438 -1.026059 -0.619500\n",
       "bbb  0.927272  0.302904 -0.032399\n",
       "ccc -0.264273 -0.386314 -0.217601\n",
       "ddd -0.871858 -0.348382  1.100491"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cat PythonForDataAnalysis/ex4.csv\n",
    "result = pd.read_csv('PythonForDataAnalysis/ex4.csv', sep = '\\s+')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern can be more intricate than this. We can have `nan` values, that we can manage by specifying `na_values=` that will replace the `nan` values by whatever we tell it to do. The list of things availables are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers:\n",
      "\n",
      "read_csv(filepath_or_buffer, sep=',', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='\"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=False, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=False, compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None)\n",
      "    Read CSV (comma-separated) file into DataFrame\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the `online docs for IO Tools\n",
      "    <http://pandas.pydata.org/pandas-docs/stable/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, pathlib.Path, py._path.local.LocalPath or any object with a read() method (such as a file handle or StringIO)\n",
      "        The string could be a URL. Valid URL schemes include http, ftp, s3, and\n",
      "        file. For file URLs, a host is expected. For instance, a local file could\n",
      "        be file ://localhost/path/to/table.csv\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used automatically. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``\n",
      "    delimiter : str, default ``None``\n",
      "        Alternative argument name for sep.\n",
      "    delim_whitespace : boolean, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    \n",
      "        .. versionadded:: 0.18.1 support for the Python parser.\n",
      "    \n",
      "    header : int or list of ints, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the data.\n",
      "        Default behavior is as if set to 0 if no ``names`` passed, otherwise\n",
      "        ``None``. Explicitly pass ``header=0`` to be able to replace existing\n",
      "        names. The header can be a list of integers that specify row locations for\n",
      "        a multi-index on the columns e.g. [0,1,3]. Intervening rows that are not\n",
      "        specified will be skipped (e.g. 2 in this example is skipped). Note that\n",
      "        this parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so header=0 denotes the first line of data\n",
      "        rather than the first line of the file.\n",
      "    names : array-like, default None\n",
      "        List of column names to use. If file contains no header row, then you\n",
      "        should explicitly pass header=None. Duplicates in this list are not\n",
      "        allowed unless mangle_dupe_cols=True, which is the default.\n",
      "    index_col : int or sequence or False, default None\n",
      "        Column to use as the row labels of the DataFrame. If a sequence is given, a\n",
      "        MultiIndex is used. If you have a malformed file with delimiters at the end\n",
      "        of each line, you might consider index_col=False to force pandas to _not_\n",
      "        use the first column as the index (row names)\n",
      "    usecols : array-like or callable, default None\n",
      "        Return a subset of the columns. If array-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). For example, a valid array-like\n",
      "        `usecols` parameter would be [0, 1, 2] or ['foo', 'bar', 'baz'].\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    as_recarray : boolean, default False\n",
      "        DEPRECATED: this argument will be removed in a future version. Please call\n",
      "        `pd.read_csv(...).to_records()` instead.\n",
      "    \n",
      "        Return a NumPy recarray instead of a DataFrame after parsing the data.\n",
      "        If set to True, this option takes precedence over the `squeeze` parameter.\n",
      "        In addition, as row indices are not available in such a format, the\n",
      "        `index_col` parameter will be ignored.\n",
      "    squeeze : boolean, default False\n",
      "        If the parsed data only contains one column then return a Series\n",
      "    prefix : str, default None\n",
      "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "    mangle_dupe_cols : boolean, default True\n",
      "        Duplicate columns will be specified as 'X.0'...'X.N', rather than\n",
      "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "        are duplicate names in the columns.\n",
      "    dtype : Type name or dict of column -> type, default None\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n",
      "        Use `str` or `object` to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    engine : {'c', 'python'}, optional\n",
      "        Parser engine to use. The C engine is faster while the python engine is\n",
      "        currently more feature-complete.\n",
      "    converters : dict, default None\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels\n",
      "    true_values : list, default None\n",
      "        Values to consider as True\n",
      "    false_values : list, default None\n",
      "        Values to consider as False\n",
      "    skipinitialspace : boolean, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like or integer or callable, default None\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c')\n",
      "    skip_footer : int, default 0\n",
      "        DEPRECATED: use the `skipfooter` parameter instead, as they are identical\n",
      "    nrows : int, default None\n",
      "        Number of rows of file to read. Useful for reading pieces of large files\n",
      "    na_values : scalar, str, list-like, or dict, default None\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'nan'`.\n",
      "    keep_default_na : bool, default True\n",
      "        If na_values are specified and keep_default_na is False the default NaN\n",
      "        values are overridden, otherwise they're appended to.\n",
      "    na_filter : boolean, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file\n",
      "    verbose : boolean, default False\n",
      "        Indicate number of NA values placed in non-numeric columns\n",
      "    skip_blank_lines : boolean, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values\n",
      "    parse_dates : boolean or list of ints or names or list of lists or dict, default False\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of ints or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call result\n",
      "          'foo'\n",
      "    \n",
      "        If a column or index contains an unparseable date, the entire column or\n",
      "        index will be returned unaltered as an object data type. For non-standard\n",
      "        datetime parsing, use ``pd.to_datetime`` after ``pd.read_csv``\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : boolean, default False\n",
      "        If True and parse_dates is enabled, pandas will attempt to infer the format\n",
      "        of the datetime strings in the columns, and if it can be inferred, switch\n",
      "        to a faster method of parsing them. In some cases this can increase the\n",
      "        parsing speed by 5-10x.\n",
      "    keep_date_col : boolean, default False\n",
      "        If True and parse_dates specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, default None\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call date_parser in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by parse_dates into a single array\n",
      "        and pass that; and 3) call date_parser once for each row using one or more\n",
      "        strings (corresponding to the columns defined by parse_dates) as arguments.\n",
      "    dayfirst : boolean, default False\n",
      "        DD/MM format dates, international and European format\n",
      "    iterator : boolean, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, default None\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer', then use gzip,\n",
      "        bz2, zip or xz if filepath_or_buffer is a string ending in '.gz', '.bz2',\n",
      "        '.zip', or 'xz', respectively, and no decompression otherwise. If using\n",
      "        'zip', the ZIP file must contain only one data file to be read in.\n",
      "        Set to None for no decompression.\n",
      "    \n",
      "        .. versionadded:: 0.18.1 support for 'zip' and 'xz' compression.\n",
      "    \n",
      "    thousands : str, default None\n",
      "        Thousands separator\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    float_precision : string, default None\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are `None` for the ordinary converter,\n",
      "        `high` for the high-precision converter, and `round_trip` for the\n",
      "        round-trip converter.\n",
      "    lineterminator : str (length 1), default None\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : boolean, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), default None\n",
      "        One-character string used to escape delimiter when quoting is QUOTE_NONE.\n",
      "    comment : str, default None\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if comment='#', parsing '#empty\\na,b,c\\n1,2,3'\n",
      "        with `header=0` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, default None\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_\n",
      "    dialect : str or csv.Dialect instance, default None\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    tupleize_cols : boolean, default False\n",
      "        Leave a list of tuples on columns as is (default is to convert to\n",
      "        a Multi Index on the columns)\n",
      "    error_bad_lines : boolean, default True\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will dropped from the DataFrame that is\n",
      "        returned.\n",
      "    warn_bad_lines : boolean, default True\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output.\n",
      "    low_memory : boolean, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser)\n",
      "    buffer_lines : int, default None\n",
      "        DEPRECATED: this argument will be removed in a future version because its\n",
      "        value is not respected by the parser\n",
      "    compact_ints : boolean, default False\n",
      "        DEPRECATED: this argument will be removed in a future version\n",
      "    \n",
      "        If compact_ints is True, then for any column that is of integer dtype,\n",
      "        the parser will attempt to cast it as the smallest integer dtype possible,\n",
      "        either signed or unsigned depending on the specification from the\n",
      "        `use_unsigned` parameter.\n",
      "    use_unsigned : boolean, default False\n",
      "        DEPRECATED: this argument will be removed in a future version\n",
      "    \n",
      "        If integer columns are being compacted (i.e. `compact_ints=True`), specify\n",
      "        whether the column should be compacted to the smallest signed or unsigned\n",
      "        integer dtype.\n",
      "    memory_map : boolean, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    result : DataFrame or TextParser\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is worth mentioning that we can store the results in a CSV file easily. We can do it by importing the CSV package and specify a lot of things or do it straightaway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|A|B|C\r\n",
      "aaa|-0.264438|-1.026059|-0.6195\r\n",
      "bbb|0.927272|0.302904|-0.032399000000000004\r\n",
      "ccc|-0.264273|-0.386314|-0.21760100000000002\r\n",
      "ddd|-0.871858|-0.348382|1.1004909999999999\r\n"
     ]
    }
   ],
   "source": [
    "result.to_csv('PythonForDataAnalysis/out1.csv', sep='|')\n",
    "!cat PythonForDataAnalysis/out1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Read JSON\n",
    "JSON (short for JavaScript Object Notation) has become one of the standard formats for sending data by HTTP request between web browsers and other applications. It is a much more flexible data format than a tabular text form like CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#JSON object example:\n",
    "obj = \"\"\"{\"name\": \"Wes\",\n",
    "          \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n",
    "          \"pet\": null,\n",
    "          \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},\n",
    "                       {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]\n",
    "         }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pet': None, 'name': 'Wes', 'siblings': [{'age': 25, 'pet': 'Zuko', 'name': 'Scott'}, {'age': 33, 'pet': 'Cisco', 'name': 'Katie'}], 'places_lived': ['United States', 'Spain', 'Germany']}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scott</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Katie</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  age\n",
       "0  Scott   25\n",
       "1  Katie   33"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if we have imported the json module! Just to be sure.\n",
    "if 'json' not in sys.modules.keys(): import json\n",
    "    \n",
    "#Convert it to python dictionary\n",
    "result = json.loads(obj)\n",
    "print(result)\n",
    "\n",
    "#Convert it back to JSON format:\n",
    "asjson = json.dumps(result)\n",
    "\n",
    "#Obtain a DataFrame (as an example) out of the siblings variable\n",
    "siblings = pd.DataFrame(result['siblings'], columns=['name', 'age'])\n",
    "siblings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data from the JSON can be stored as conviniently as we want it to be.\n",
    "\n",
    "### 6.3 Read XML/HTML\n",
    "The basic idea is that we need to use the `request` package, which is one of the easiest to use, and then use `BeautifulSoup`, to access the HTML/XML format in a easy way. When we have retreived the data, we will need to work a bit to obtain the information that we want and then we can pass it to a panda's structure. We will work an example of _Yahoo! Finances_ database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#Check if we have imported the requests module! Just to be sure.\n",
    "if 'requests' not in sys.modules.keys(): import requests\n",
    "if 'bs4' not in sys.modules.keys(): from bs4 import BeautifulSoup as BS\n",
    "\n",
    "#Obtain the raw data:\n",
    "rawweb = requests.get('https://finance.yahoo.com/quote/AAPL/options?ltr=1')\n",
    "print(rawweb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requests.get( )` returns a _response [code]_ which is an object that contains all the information of the webpage. The code tells us if it could read it (code 200) or if something happened. In order to work with it, we could transform it into a json format but unfortunately this webpage can't be transformed into that format. For cases like that, the best idea is to use a parser such as bs4 (`BeautifulSoup`) and work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL Option Chain | Apple Inc. Stock - Yahoo Finance\n",
      "https://www.yahoo.com/\n",
      " https://mail.yahoo.com/?.intl=us&.lang=en-US\n",
      " https://www.flickr.com/\n",
      " https://www.tumblr.com/\n",
      " https://www.yahoo.com/news/\n",
      " http://sports.yahoo.com/\n",
      " http://finance.yahoo.com/\n",
      " https://www.yahoo.com/celebrity/\n",
      " https://answers.yahoo.com/\n",
      " https://groups.yahoo.com/\n",
      "\n",
      "Search\n",
      " Contract NameLast Trade DateStrikeLast PriceBidAskChange% ChangeVolumeOpen InterestImplied VolatilityAAPL170825C000950002017-08-16 11:52AM EDT95... (there is more!)\n"
     ]
    }
   ],
   "source": [
    "#Transform it into a BeautifulSoup object which is a easy manipulable parser\n",
    "web_parsed = BS(rawweb.text, 'html.parser')\n",
    "\n",
    "#Let's check the title:\n",
    "print(*web_parsed.title)\n",
    "\n",
    "#Let's find all the links in the page:\n",
    "links = web_parsed.find_all('a')\n",
    "print(*['{}\\n'.format(i['href']) for i in links[:10]])\n",
    "\n",
    "#...or the tables with the interesting data:\n",
    "tables = web_parsed.find_all('table')\n",
    "print(*['{}\\n'.format(i.get_text()[:144]) \n",
    "        if len(i.get_text()) < 144 \n",
    "        else '{}... (there is more!)'.format(i.get_text()[:144]) \n",
    "        for i in tables[:2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the tables, we can see that the one with information is table 1. Now that we have obtained the information from the URL, let's try to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contract name</th>\n",
       "      <th>Last trade date</th>\n",
       "      <th>Strike</th>\n",
       "      <th>Last price</th>\n",
       "      <th>Bid</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Change</th>\n",
       "      <th>% change</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open interest</th>\n",
       "      <th>Implied volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL170825C00095000</td>\n",
       "      <td>2017-08-16 11:52AM EDT</td>\n",
       "      <td>95.00</td>\n",
       "      <td>67.02</td>\n",
       "      <td>62.10</td>\n",
       "      <td>63.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>146.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL170825C00100000</td>\n",
       "      <td>2017-08-14 12:03PM EDT</td>\n",
       "      <td>100.00</td>\n",
       "      <td>59.15</td>\n",
       "      <td>57.10</td>\n",
       "      <td>58.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>132.81%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL170825C00110000</td>\n",
       "      <td>2017-08-14 2:05PM EDT</td>\n",
       "      <td>110.00</td>\n",
       "      <td>50.05</td>\n",
       "      <td>47.10</td>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>107.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL170825C00115000</td>\n",
       "      <td>2017-08-16 9:30AM EDT</td>\n",
       "      <td>115.00</td>\n",
       "      <td>46.87</td>\n",
       "      <td>42.10</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>94.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL170825C00120000</td>\n",
       "      <td>2017-08-18 12:59PM EDT</td>\n",
       "      <td>120.00</td>\n",
       "      <td>39.20</td>\n",
       "      <td>37.15</td>\n",
       "      <td>38.00</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-3.99%</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>87.89%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Contract name         Last trade date  Strike Last price    Bid  \\\n",
       "0  AAPL170825C00095000  2017-08-16 11:52AM EDT   95.00      67.02  62.10   \n",
       "1  AAPL170825C00100000  2017-08-14 12:03PM EDT  100.00      59.15  57.10   \n",
       "2  AAPL170825C00110000   2017-08-14 2:05PM EDT  110.00      50.05  47.10   \n",
       "3  AAPL170825C00115000   2017-08-16 9:30AM EDT  115.00      46.87  42.10   \n",
       "4  AAPL170825C00120000  2017-08-18 12:59PM EDT  120.00      39.20  37.15   \n",
       "\n",
       "     Ask Change % change Volume Open interest Implied volatility  \n",
       "0  63.00   0.00      NaN      1             1            146.09%  \n",
       "1  58.00   0.00      NaN      1             1            132.81%  \n",
       "2  48.00   0.00      NaN      5             6            107.03%  \n",
       "3  43.00   0.00      NaN      1             1             94.92%  \n",
       "4  38.00  -1.63   -3.99%      6             0             87.89%  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = tables[1]\n",
    "\n",
    "#Now with a bit of intelligence, we can obtain the table :)\n",
    "#   1. Get the column names:\n",
    "columns = [i.get_text().capitalize() for i in f.find_all('th')]\n",
    "\n",
    "#   2. The contract names are treated as links. Need for special synthax:\n",
    "ContractNames = [i.get_text() for x, i in enumerate(f.find_all('a')) if x%2==0]\n",
    "\n",
    "#   3. Start building the dictionary with the contract names:\n",
    "data = {columns[0]: ContractNames}\n",
    "\n",
    "#   4. Get the rest of the values:\n",
    "for i in range(1, len(columns)):\n",
    "    col = 'data-col{}'.format(i)\n",
    "    cellcontent = [i.get_text() for i in f.find_all('td', attrs={'class':col})]\n",
    "    data[columns[i]] = cellcontent\n",
    "\n",
    "#   5. Create the pandas structure in the same order as the webpage!\n",
    "finance = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "#   6. Fix the issue of using \"-\" instead of NaN for missing values:\n",
    "finance['% change'].replace(to_replace = '-', value = np.nan, inplace = True)\n",
    "finance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Database formats (mySQL,...)\n",
    "It is very common that data is stored in a database structure such as the SQL and non-SQL formats, which stand for Search Query Language. Reading the data into python is very straightforward. We can use the `sqlite3` package from the Python standard library to connect to the sqlite database and create a cursor. A `sqlite3.Cursor` object is our interface to the database, mostly throught the execute method that allows to run any SQL query on our database.\n",
    "\n",
    "First of all we can get a list of all the tables saved into the database, this is done by reading the column name from the sqlite_master metadata table with:\n",
    "\n",
    "    SELECT name FROM sqlite_master\n",
    "\n",
    "If we want everything, we just need to use the wildcard `*` instead of the `name` in the query.\n",
    "The output of the execute method is an iterator that can be used in a for loop to print the value of each row or a shortcut to directly execute the query and gather the results is the `.fetchall()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "Id                                                                       \n",
       "1             5.1           3.5            1.4           0.2  Iris-setosa\n",
       "2             4.9           3.0            1.4           0.2  Iris-setosa\n",
       "3             4.7           3.2            1.3           0.2  Iris-setosa\n",
       "4             4.6           3.1            1.5           0.2  Iris-setosa\n",
       "5             5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'sqlite3' not in sys.modules.keys(): import sqlite3\n",
    "\n",
    "#Create the cursor:\n",
    "conn = sqlite3.connect('../DDBB/iris.sqlite')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#Get the names of the tables:\n",
    "name1 = []\n",
    "for row in cursor.execute(\"SELECT name FROM sqlite_master\"):\n",
    "    name1.append(row)\n",
    "name2 = cursor.execute(\"SELECT name FROM sqlite_master\").fetchall()\n",
    "\n",
    "#Read the name of the database:\n",
    "print(*name1[0])\n",
    "\n",
    "#Instruction to read the database:\n",
    "irisdata = pd.read_sql_query(\"SELECT * FROM {}\".format(*name1[0]), conn)\n",
    "\n",
    "#Display it:\n",
    "irisdata.set_index('Id', inplace=True)\n",
    "irisdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more advanced query would have been:\n",
    "    \n",
    "    SELECT * FROM Iris WHERE Species == 'Iris-setosa'\n",
    "   \n",
    "in which we are saying _select everything from the database \"Iris\" which have \"iris-setosa\" as a tag for species_. More details on how to make SQL queries can be found in the [webpage](https://docs.python.org/3/library/sqlite3.html) or in some [tutorials](http://zetcode.com/db/sqlitepythontutorial/). `sqlite3` is extremely useful for downselecting data **before** importing them in `pandas`.\n",
    "\n",
    "For example you might have 1 TB of data in a table stored in a database on a server machine. You are interested in working on a subset of the data based on some criterion, unfortunately it would be impossible to first load data into `pandas` and then filter them, therefore we should tell the database to perform the filtering and just load into `pandas` the downsized dataset.\n",
    "\n",
    "## Chapter 7: data wrangling - clean, transform, merge and reshape\n",
    "Data contained in pandas objects can be combined together in a number of built-in ways:\n",
    "* `pandas.merge()` connects rows in DataFrames based on one or more keys. It implements database join operations, as in relational databases.\n",
    "* `pandas.concat()` glues or stacks together objects along an axis.\n",
    "* `combine_first()` instance method enables splicing together overlapping data to fill in missing values in one object with values from another.\n",
    "\n",
    "### 7.1 `merge( )` and `join( )`\n",
    "This pandas operation will perform a join between two tables based on the key table that we define. Then the basic usage is `pd.merge([table1], [table2], on = [column], how = [style])`. The types of junction are the following:\n",
    "![Join types](joins.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Inner:\n",
      "   data1 key  data2\n",
      "0      0   b      1\n",
      "1      1   b      1\n",
      "2      6   b      1\n",
      "3      2   a      0\n",
      "4      4   a      0\n",
      "5      5   a      0\n",
      "\n",
      "2. Left:\n",
      "   data1 key  data2\n",
      "0      0   b    1.0\n",
      "1      1   b    1.0\n",
      "2      2   a    0.0\n",
      "3      3   c    NaN\n",
      "4      4   a    0.0\n",
      "5      5   a    0.0\n",
      "6      6   b    1.0\n",
      "\n",
      "3. Right:\n",
      "   data1 key  data2\n",
      "0    0.0   b      1\n",
      "1    1.0   b      1\n",
      "2    6.0   b      1\n",
      "3    2.0   a      0\n",
      "4    4.0   a      0\n",
      "5    5.0   a      0\n",
      "6    NaN   d      2\n",
      "\n",
      "4. Outer:\n",
      "   data1 key  data2\n",
      "0    0.0   b    1.0\n",
      "1    1.0   b    1.0\n",
      "2    6.0   b    1.0\n",
      "3    2.0   a    0.0\n",
      "4    4.0   a    0.0\n",
      "5    5.0   a    0.0\n",
      "6    3.0   c    NaN\n",
      "7    NaN   d    2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)})\n",
    "df2 = pd.DataFrame({'key': ['a', 'b', 'd'], 'data2': range(3)})\n",
    "\n",
    "print('1. Inner:\\n{}\\n'.format(pd.merge(df1, df2, on='key')))\n",
    "print('2. Left:\\n{}\\n'.format(pd.merge(df1, df2, on='key', how = 'left')))\n",
    "print('3. Right:\\n{}\\n'.format(pd.merge(df1, df2, on='key', how = 'right')))\n",
    "print('4. Outer:\\n{}\\n'.format(pd.merge(df1, df2, on='key', how = 'outer')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the key to merge is in the index, we can merge it by passing a parameter for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  key  value  group_val\n",
      "0   a      0        3.5\n",
      "2   a      2        3.5\n",
      "3   a      3        3.5\n",
      "1   b      1        7.0\n",
      "4   b      4        7.0\n"
     ]
    }
   ],
   "source": [
    "left1 = pd.DataFrame({'key': ['a', 'b', 'a', 'a', 'b', 'c'], 'value': range(6)})\n",
    "right1 = pd.DataFrame({'group_val':[3.5,7]}, index = list('ab'))\n",
    "print(pd.merge(left1, right1, left_on = 'key', right_index = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If each table has multiple keys, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  key1 key2  lval\n",
      "0  foo  one     1\n",
      "1  foo  two     2\n",
      "2  bar  one     3\n",
      "  key1 key2  rval\n",
      "0  foo  one     4\n",
      "1  foo  one     5\n",
      "2  bar  one     6\n",
      "3  bar  two     7\n",
      "\n",
      "Merged multiple keys: \n",
      "  key1 key2  lval  rval\n",
      "0  foo  one   1.0   4.0\n",
      "1  foo  one   1.0   5.0\n",
      "2  foo  two   2.0   NaN\n",
      "3  bar  one   3.0   6.0\n",
      "4  bar  two   NaN   7.0\n",
      "\n",
      "Suffixes: \n",
      "  key1 key2_LEFT  lval key2_RIGHT  rval\n",
      "0  foo       one     1        one     4\n",
      "1  foo       one     1        one     5\n",
      "2  foo       two     2        one     4\n",
      "3  foo       two     2        one     5\n",
      "4  bar       one     3        one     6\n",
      "5  bar       one     3        two     7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left = pd.DataFrame({'key1': ['foo', 'foo', 'bar'],\n",
    "                   'key2': ['one', 'two', 'one'],\n",
    "                   'lval': [1, 2, 3]})\n",
    "right = pd.DataFrame({'key1': ['foo', 'foo', 'bar', 'bar'],\n",
    "                   'key2': ['one', 'one', 'one', 'two'],\n",
    "                   'rval': [4, 5, 6, 7]})\n",
    "print('{}\\n{}\\n'.format(left,right))\n",
    "print('Merged multiple keys: \\n{}\\n'.format(pd.merge(left, right, on=['key1', 'key2'], how='outer')))\n",
    "\n",
    "#Add suffixes if we have the same key but different values:\n",
    "print('Suffixes: \\n{}\\n'.format(pd.merge(left, right, on='key1', suffixes=('_LEFT', '_RIGHT'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to merge through indexes, we can use join (for simple cases) or if we hierarchical data, one has to indicate that the indexes are what we are looking for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple join:\n",
      "   Barcelona  Lleida  Girona  Tarragona\n",
      "a        1.0     2.0     NaN        NaN\n",
      "b        NaN     NaN     7.0        8.0\n",
      "c        3.0     4.0     9.0       10.0\n",
      "d        NaN     NaN    11.0       12.0\n",
      "e        5.0     6.0    13.0       14.0\n",
      "\n",
      "Hierarchical join:\n",
      "   data       key1  key2  one  two\n",
      "0     0  Barcelona  2015    2    3\n",
      "1     1  Barcelona  2016    0    1\n",
      "3     3     Lleida  2016    8    9\n",
      "4     4     Lleida  2017   10   11\n"
     ]
    }
   ],
   "source": [
    "left2 = pd.DataFrame([[1., 2.], [3., 4.], [5., 6.]],\n",
    "                  index=['a', 'c', 'e'],\n",
    "                  columns=['Barcelona', 'Lleida'])\n",
    "right2 = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]], \n",
    "                   index=['b', 'c', 'd', 'e'], \n",
    "                   columns=['Girona', 'Tarragona'])\n",
    "print('Simple join:\\n{}\\n'.format(left2.join(right2, how = 'outer')))\n",
    "\n",
    "left3 = pd.DataFrame({'key1':['Barcelona','Barcelona','Barcelona','Lleida','Lleida'],\n",
    "                      'key2':[2015,2016,2017,2016,2017],\n",
    "                      'data': np.arange(5)})\n",
    "right3 = pd.DataFrame(np.arange(12).reshape((6,2)),\n",
    "                     index = [['Barcelona','Barcelona','Lleida','Lleida','Lleida','Lleida'],\n",
    "                             [2016, 2015, 2015, 2015, 2016, 2017]],\n",
    "                     columns = ['one', 'two'])\n",
    "print('Hierarchical join:\\n{}'.format(pd.merge(left3, right3, left_on=['key1','key2'], right_index=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 `concat( )`\n",
    "As described above, it is used to stack the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack the data as rows:\n",
      "a    0\n",
      "b    1\n",
      "c    2\n",
      "d    3\n",
      "e    4\n",
      "f    5\n",
      "g    6\n",
      "dtype: int64\n",
      "\n",
      "Stack the data as columns (creates DataFrame):\n",
      "     0    1    2\n",
      "a  0.0  NaN  NaN\n",
      "b  1.0  NaN  NaN\n",
      "c  NaN  2.0  NaN\n",
      "d  NaN  3.0  NaN\n",
      "e  NaN  4.0  NaN\n",
      "f  NaN  NaN  5.0\n",
      "g  NaN  NaN  6.0\n",
      "\n",
      "Outer join:\n",
      "     0  1\n",
      "a  0.0  0\n",
      "b  1.0  5\n",
      "f  NaN  5\n",
      "g  NaN  6\n",
      "\n",
      "Inner join:\n",
      "   0  1\n",
      "a  0  0\n",
      "b  1  5\n"
     ]
    }
   ],
   "source": [
    "s1 = pd.Series([0, 1], index=['a', 'b'])\n",
    "s2 = pd.Series([2, 3, 4], index=['c', 'd', 'e'])\n",
    "s3 = pd.Series([5, 6], index=['f', 'g'])\n",
    "print('Stack the data as rows:\\n{}\\n'.format(pd.concat([s1, s2, s3])))\n",
    "print('Stack the data as columns (creates DataFrame):\\n{}\\n'.format(pd.concat([s1, s2, s3], axis = 1)))\n",
    "\n",
    "#We can pass the \"how\" parameter here too but it's called \"join\":\n",
    "s4 = pd.concat([s1 * 5, s3])\n",
    "print('Outer join:\\n{}\\n'.format(pd.concat([s1, s4], axis=1)))\n",
    "print('Inner join:\\n{}'.format(pd.concat([s1, s4], axis=1, join='inner')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When concatenating the data, we might want to keep an origin reference of the indexes, which we can achieve by passing the parameter `keys = []` or maybe we don't need to keep the indexes, which we can achieve by `ignore_index = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set the axis keys:\n",
      "    S1  S4\n",
      "a  0.0   0\n",
      "b  1.0   5\n",
      "f  NaN   5\n",
      "g  NaN   6\n",
      "\n",
      "Forget the indexes:\n",
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    5\n",
      "4    5\n",
      "5    6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Set the axis keys:\\n{}\\n'.format(pd.concat([s1, s4], axis=1, keys=['S1', 'S4'])))\n",
    "print('Forget the indexes:\\n{}'.format(pd.concat([s1, s4], axis=0, ignore_index=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 `combine_first( )`\n",
    "You may have two datasets whose indexes overlap in full or part. As a motivating example, consider `np.where()` function, which expressed a vectorized if-else. It works with three parameters:\n",
    " * Condition to check.\n",
    " * Value if true.\n",
    " * Value if false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original series\n",
      "#A#\n",
      "f    NaN\n",
      "e    2.5\n",
      "d    NaN\n",
      "c    3.5\n",
      "b    4.5\n",
      "a    NaN\n",
      "dtype: float64\n",
      "#B#\n",
      "f    0.0\n",
      "e    1.0\n",
      "d    2.0\n",
      "c    3.0\n",
      "b    4.0\n",
      "a    NaN\n",
      "dtype: float64\n",
      "\n",
      "Resulted vector:\n",
      "[ 0.   2.5  2.   3.5  4.5  nan]\n",
      "\n",
      "Same with pandas but outputs a Series:\n",
      "f    0.0\n",
      "e    2.5\n",
      "d    2.0\n",
      "c    3.5\n",
      "b    4.5\n",
      "a    NaN\n",
      "dtype: float64\n",
      "\n",
      "DF combine first:\n",
      "     a    b     c\n",
      "0  1.0  NaN   2.0\n",
      "1  4.0  2.0   6.0\n",
      "2  5.0  4.0  10.0\n",
      "3  3.0  6.0  14.0\n",
      "4  7.0  8.0   NaN\n"
     ]
    }
   ],
   "source": [
    "a = pd.Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan],\n",
    "              index=['f', 'e', 'd', 'c', 'b', 'a'])\n",
    "b = pd.Series(np.arange(len(a), dtype=np.float64),\n",
    "              index=['f', 'e', 'd', 'c', 'b', 'a'])\n",
    "b[-1] = np.nan\n",
    "\n",
    "print('Original series\\n#A#\\n{}\\n#B#\\n{}\\n'.format(a,b))\n",
    "\n",
    "#Find the NaN values in a, then if they are, use the value in b. Otherwise keep the value in a:\n",
    "print('Resulted vector:\\n{}\\n'.format(np.where(pd.isnull(a),b,a)))\n",
    "print('Same with pandas but outputs a Series:\\n{}\\n'.format(a.combine_first(b)))\n",
    "\n",
    "#In DataFrames it works similarly column by column:\n",
    "df1 = pd.DataFrame({'a': [1., np.nan, 5., np.nan],\n",
    "                 'b': [np.nan, 2., np.nan, 6.],\n",
    "                 'c': range(2, 18, 4)})\n",
    "df2 = pd.DataFrame({'a': [5., 4., np.nan, 3., 7.],\n",
    "                 'b': [np.nan, 3., 4., 6., 8.]})\n",
    "print('DF combine first:\\n{}'.format(df1.combine_first(df2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Pivoting\n",
    "Sometimes we need to re-organize the data in a easy way and perhaps get it stacked. We can do it with `pivot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data:\n",
      "        date     item     value\n",
      "0 1959-03-31  realgdp  2710.349\n",
      "1 1959-03-31     infl     0.000\n",
      "2 1959-03-31    unemp     5.800\n",
      "3 1959-06-30  realgdp  2778.801\n",
      "4 1959-06-30     infl     2.340\n",
      "5 1959-06-30    unemp     5.100\n",
      "6 1959-09-30  realgdp  2775.488\n",
      "7 1959-09-30     infl     2.740\n",
      "8 1959-09-30    unemp     5.300\n",
      "9 1959-12-31  realgdp  2785.204\n",
      "\n",
      "Pivoted data:\n",
      "item        infl   realgdp  unemp\n",
      "date                             \n",
      "1959-03-31  0.00  2710.349    5.8\n",
      "1959-06-30  2.34  2778.801    5.1\n",
      "1959-09-30  2.74  2775.488    5.3\n",
      "1959-12-31  0.27  2785.204    5.6\n",
      "1960-03-31  2.31  2847.699    5.2\n",
      "1960-06-30  0.14  2834.390    5.2\n",
      "1960-09-30  2.70  2839.022    5.6\n",
      "1960-12-31  1.21  2802.616    6.3\n",
      "1961-03-31 -0.40  2819.264    6.8\n",
      "1961-06-30  1.47  2872.005    7.0\n",
      "\n",
      "If we ignore the last parameter, we can have more keys:\n",
      "           value                    value2                    \n",
      "item        infl   realgdp unemp      infl   realgdp     unemp\n",
      "date                                                          \n",
      "1959-03-31  0.00  2710.349   5.8 -0.998673 -0.584986  1.125747\n",
      "1959-06-30  2.34  2778.801   5.1 -0.033017 -0.524073  0.168893\n",
      "1959-09-30  2.74  2775.488   5.3 -1.162704  0.581302 -1.136544\n",
      "1959-12-31  0.27  2785.204   5.6 -1.010633  1.322869  0.408588\n",
      "1960-03-31  2.31  2847.699   5.2 -0.566992  0.366799 -0.891232\n",
      "1960-06-30  0.14  2834.390   5.2 -0.264401 -0.402460  0.473483\n",
      "1960-09-30  2.70  2839.022   5.6 -1.540976  1.643118  0.921180\n",
      "1960-12-31  1.21  2802.616   6.3  0.797818 -0.027179  0.250453\n",
      "1961-03-31 -0.40  2819.264   6.8 -1.873266  0.050468 -0.723831\n",
      "1961-06-30  1.47  2872.005   7.0  0.622799  0.619655  0.151911\n"
     ]
    }
   ],
   "source": [
    "#Read the data:\n",
    "data = pd.read_csv('../Pythoneo/PythonForDataAnalysis/ex5.csv')\n",
    "periods = pd.PeriodIndex(year=data.year, quarter=data.quarter, name='date')\n",
    "data = pd.DataFrame(data.to_records(),\n",
    "                 columns=pd.Index(['realgdp', 'infl', 'unemp'], name='item'),\n",
    "                 index=periods.to_timestamp('D', 'end'))\n",
    "\n",
    "#Results:\n",
    "ldata = data.stack().reset_index().rename(columns={0: 'value'})\n",
    "print('Raw data:\\n{}\\n'.format(ldata.head(10)))\n",
    "wdata = ldata.pivot('date', 'item', 'value')\n",
    "print('Pivoted data:\\n{}\\n'.format(wdata.head(10)))\n",
    "ldata['value2'] = np.random.randn(len(ldata))\n",
    "wdata2 = ldata.pivot('date', 'item')\n",
    "print('If we ignore the last parameter, we can have more keys:\\n{}'.format(wdata2.head(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Data transformation\n",
    "We can do a set of things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\tREMOVE DUPLICATES\n",
      "1. Oritinal data:\n",
      "    k1  k2\n",
      "0  one   1\n",
      "1  one   1\n",
      "2  one   2\n",
      "3  two   3\n",
      "4  two   3\n",
      "5  two   4\n",
      "6  two   4\n",
      "\n",
      "2. No duplicates:\n",
      "    k1  k2\n",
      "0  one   1\n",
      "2  one   2\n",
      "3  two   3\n",
      "5  two   4\n",
      "\n",
      "3. Drop duplicates of a column:\n",
      "    k1  k2  v1\n",
      "0  one   1   0\n",
      "3  two   3   3\n",
      "\n",
      "4. Keep the last duplicated value instead of the first:\n",
      "    k1  k2  v1\n",
      "1  one   1   1\n",
      "2  one   2   2\n",
      "4  two   3   4\n",
      "6  two   4   6\n",
      "\n",
      "\t\t\t\t\t\tFUNCTION MAPPING\n",
      "1. Manual mapping:\n",
      "0       pig\n",
      "1       pig\n",
      "2       pig\n",
      "3       cow\n",
      "4       cow\n",
      "5       pig\n",
      "6       cow\n",
      "7       pig\n",
      "8    salmon\n",
      "Name: food, dtype: object\n",
      "\n",
      "2. Lambda mapping:\n",
      "0       pig\n",
      "1       pig\n",
      "2       pig\n",
      "3       cow\n",
      "4       cow\n",
      "5       pig\n",
      "6       cow\n",
      "7       pig\n",
      "8    salmon\n",
      "Name: food, dtype: object\n",
      "\n",
      "\t\t\t\t\tREPLACING VALUES AND INDEX RE-NAMING\n",
      "Already seen in cells \"In [4]:\" & \"In [5]:\"\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tDATA BINNING\n",
      "1. Category codes:[0 0 0 1 0 0 2 1 3 2 2 1]\n",
      "\n",
      "2. Category categories:\n",
      "IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]]\n",
      "              closed='right',\n",
      "              dtype='interval[int64]')\n",
      "\n",
      "3. Category categories with right bound open:\n",
      "IntervalIndex([[18, 25), [25, 35), [35, 60), [60, 100)]\n",
      "              closed='left',\n",
      "              dtype='interval[int64]')\n",
      "\n",
      "4. Counting the categories:\n",
      "(18, 25]     5\n",
      "(35, 60]     3\n",
      "(25, 35]     3\n",
      "(60, 100]    1\n",
      "dtype: int64\n",
      "\n",
      "5. Naming the category:\n",
      "Index(['Young', 'YoungAdult', 'MiddleAged', 'Senior'], dtype='object')\n",
      "\n",
      "6. Cutting by standard quantiles:\n",
      "(0.649, 4.025]       250\n",
      "(-0.0409, 0.649]     250\n",
      "(-0.691, -0.0409]    250\n",
      "(-4.39, -0.691]      250\n",
      "dtype: int64\n",
      "\n",
      "7. Cutting by custom quantiles:\n",
      "(-0.0409, 1.245]     400\n",
      "(-1.206, -0.0409]    400\n",
      "(1.245, 4.025]       100\n",
      "(-4.39, -1.206]      100\n",
      "dtype: int64\n",
      "\n",
      "\t\t\t\t\t\tDETECTING OUTLIERS\n",
      "Already seen in cells \"In [4]:\" & \"In [5]:\" (boolean indexing)\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tRANDOM SUBSAMPLING\n",
      "1. Original data:\n",
      "    0   1   2   3\n",
      "0   0   1   2   3\n",
      "1   4   5   6   7\n",
      "2   8   9  10  11\n",
      "3  12  13  14  15\n",
      "4  16  17  18  19\n",
      "\n",
      "2. Random subsample of 2 rows:\n",
      "    0   1   2   3\n",
      "4  16  17  18  19\n",
      "1   4   5   6   7\n",
      "\n",
      "\t\t\t\t\t\tVALUE MATRIX\n",
      "1. Original data:\n",
      "   data1 key\n",
      "0      0   b\n",
      "1      1   b\n",
      "2      2   a\n",
      "3      3   c\n",
      "4      4   a\n",
      "5      5   b\n",
      "\n",
      "2. Matrix value:\n",
      "   a  b  c\n",
      "0  0  1  0\n",
      "1  0  1  0\n",
      "2  1  0  0\n",
      "3  0  0  1\n",
      "4  1  0  0\n",
      "5  0  1  0\n",
      "\n",
      "3. Summary dataframe:\n",
      "   data1  key_a  key_b  key_c\n",
      "0      0      0      1      0\n",
      "1      1      0      1      0\n",
      "2      2      1      0      0\n",
      "3      3      0      0      1\n",
      "4      4      1      0      0\n",
      "5      5      0      1      0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/al/.local/lib/python3.5/site-packages/ipykernel_launcher.py:76: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id                                       1\n",
      "title                           Toy Story (1995)\n",
      "genres               Animation|Children's|Comedy\n",
      "Genre_Action                                   0\n",
      "Genre_Adventure                                0\n",
      "Genre_Animation                                1\n",
      "Genre_Children's                               1\n",
      "Genre_Comedy                                   1\n",
      "Genre_Crime                                    0\n",
      "Genre_Documentary                              0\n",
      "Genre_Drama                                    0\n",
      "Genre_Fantasy                                  0\n",
      "Genre_Film-Noir                                0\n",
      "Genre_Horror                                   0\n",
      "Genre_Musical                                  0\n",
      "Genre_Mystery                                  0\n",
      "Genre_Romance                                  0\n",
      "Genre_Sci-Fi                                   0\n",
      "Genre_Thriller                                 0\n",
      "Genre_War                                      0\n",
      "Genre_Western                                  0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Remove duplicates:\n",
    "print('{}Remove duplicates'.upper().format('\\t'*6))\n",
    "data = pd.DataFrame({'k1': ['one'] * 3 + ['two'] * 4,\n",
    "                  'k2': [1, 1, 2, 3, 3, 4, 4]})\n",
    "print('1. Oritinal data:\\n{}\\n'.format(data))\n",
    "print('2. No duplicates:\\n{}\\n'.format(data.drop_duplicates()))\n",
    "\n",
    "data['v1'] = range(7)\n",
    "print('3. Drop duplicates of a column:\\n{}\\n'.format(data.drop_duplicates(['k1'])))\n",
    "print('4. Keep the last duplicated value instead of the first:\\n{}\\n'.format(\n",
    "    data.drop_duplicates(['k1', 'k2'], keep='last')))\n",
    "\n",
    "#Map a function:\n",
    "print('{}Function mapping'.upper().format('\\t'*6))\n",
    "data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami',\n",
    "                           'corned beef', 'Bacon', 'pastrami', 'honey ham',\n",
    "                           'nova lox'],\n",
    "                      'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\n",
    "meat_to_animal = {'bacon': 'pig',\n",
    "                  'pulled pork': 'pig',\n",
    "                  'pastrami': 'cow',\n",
    "                  'corned beef': 'cow',\n",
    "                  'honey ham': 'pig',\n",
    "                  'nova lox': 'salmon'}\n",
    "\n",
    "print('1. Manual mapping:\\n{}\\n'.format(data['food'].map(str.lower).map(meat_to_animal)))\n",
    "print('2. Lambda mapping:\\n{}\\n'.format(data['food'].map(lambda x: meat_to_animal[x.lower()])))\n",
    "\n",
    "#Replacing values and index re-naming:\n",
    "print('{}Replacing values and index re-naming'.upper().format('\\t'*5))\n",
    "print('Already seen in cells \"In [4]:\" & \"In [5]:\"\\n\\n')\n",
    "\n",
    "#Data binning:\n",
    "print('{}Data binning'.upper().format('\\t'*6))\n",
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\n",
    "bins = [18, 25, 35, 60, 100]\n",
    "cats = pd.cut(ages, bins)\n",
    "cats2 = pd.cut(ages,bins,right=False)\n",
    "cats3 = pd.cut(ages,bins,labels=['Young', 'YoungAdult', 'MiddleAged', 'Senior'])\n",
    "data = np.random.randn(1000)\n",
    "cats4 = pd.qcut(data,4)\n",
    "cats5 = pd.qcut(data,[0, 0.1, 0.5, 0.9, 1])\n",
    "print('1. Category codes:{}\\n'.format(cats.codes))\n",
    "print('2. Category categories:\\n{}\\n'.format(cats.categories))\n",
    "print('3. Category categories with right bound open:\\n{}\\n'.format(cats2.categories))\n",
    "print('4. Counting the categories:\\n{}\\n'.format(pd.value_counts(cats)))\n",
    "print('5. Naming the category:\\n{}\\n'.format(cats3.categories))\n",
    "print('6. Cutting by standard quantiles:\\n{}\\n'.format(pd.value_counts(cats4)))\n",
    "print('7. Cutting by custom quantiles:\\n{}\\n'.format(pd.value_counts(cats5)))\n",
    "\n",
    "#Replacing values and index re-naming:\n",
    "print('{}Detecting outliers'.upper().format('\\t'*6))\n",
    "print('Already seen in cells \"In [4]:\" & \"In [5]:\" (boolean indexing)\\n\\n')\n",
    "\n",
    "#Easy random subsampling:\n",
    "print('{}Random subsampling'.upper().format('\\t'*6))\n",
    "df = pd.DataFrame(np.arange(20).reshape(5,4))\n",
    "sampler = np.random.permutation(5)\n",
    "print('1. Original data:\\n{}\\n'.format(df))\n",
    "print('2. Random subsample of 2 rows:\\n{}\\n'.format(df.take(sampler)[:2]))\n",
    "\n",
    "#Value matrix: to find the places where our data has values:\n",
    "print('{}Value matrix'.upper().format('\\t'*6))\n",
    "df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\n",
    "                'data1': range(6)})\n",
    "print('1. Original data:\\n{}\\n'.format(df))\n",
    "print('2. Matrix value:\\n{}\\n'.format(pd.get_dummies(df['key'])))\n",
    "dummies = pd.get_dummies(df['key'], prefix='key')\n",
    "df_with_dummy = df[['data1']].join(dummies)\n",
    "print('3. Summary dataframe:\\n{}\\n'.format(df_with_dummy))\n",
    "\n",
    "#Real life example:\n",
    "    #Load and obtain genres\n",
    "mnames = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_table('../Pythoneo/PythonForDataAnalysis/movielens.dat', sep='::', header=None,\n",
    "                        names=mnames)\n",
    "genre_iter = [set(x.split('|')) for x in movies.genres]\n",
    "genres = sorted(set.union(*genre_iter))\n",
    "    #Preallocate matrix for speed and fill it with ones if the genre appears\n",
    "dummies = pd.DataFrame(np.zeros((len(movies), len(genres))), columns = genres)\n",
    "for i,gen in enumerate(movies.genres):\n",
    "    dummies.loc[i, gen.split('|')] = 1\n",
    "movies_windic = movies.join(dummies.add_prefix('Genre_'))\n",
    "print(movies_windic.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: Data aggregation and group operations\n",
    "Chapter 8 is only about drawing with matplotlib, so it is skipped. This section is fully dedicated to the `groupby( )` method. It is useful because it allows us to:\n",
    " 1. Split a pandas object into pieces using one or more keys (in the form of functions, arrays, or DataFrame column names).\n",
    " 2. Computing group summary statistics, like count, mean, or standard deviation, or a user-defined function.\n",
    " 3. Apply a varying set of functions to each column of a DataFrame.\n",
    " 4. Apply within-group transformations or other manipulations, like normalization, linear regression, rank, or subset selection.\n",
    " 5. Compute pivot tables and cross-tabulations.\n",
    " 6. Perform quantile analysis and other data-derived group analyses\n",
    "\n",
    "The basic mechanism is to split the data - apply operation - combine the result. The output of a `groupby` method will always be an object. It hasn't computed anything yet and we have to request what we want from it as in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. DataFrame:\n",
      "      data1     data2 key1 key2\n",
      "0  0.247948 -0.027877    a  one\n",
      "1  1.961050  0.518072    a  two\n",
      "2 -2.167729 -0.823758    b  one\n",
      "3  0.643310 -0.664134    b  two\n",
      "4  0.034184  0.081383    a  one\n",
      "\n",
      "2. Group object: <pandas.core.groupby.SeriesGroupBy object at 0x7f0948c653c8>\n",
      "\n",
      "3. Absolutes by group:\n",
      "0    0.247948\n",
      "1    1.961050\n",
      "2    2.167729\n",
      "3    0.643310\n",
      "4    0.034184\n",
      "Name: data1, dtype: float64\n",
      "\n",
      "4. Sum by group:\n",
      "key1\n",
      "a    2.243182\n",
      "b   -1.524420\n",
      "Name: data1, dtype: float64\n",
      "\n",
      "5. Means by group:\n",
      "key1\n",
      "a    0.747727\n",
      "b   -0.762210\n",
      "Name: data1, dtype: float64\n",
      "\n",
      "6. Means by groupS:\n",
      "key1  key2\n",
      "a     one     0.141066\n",
      "      two     1.961050\n",
      "b     one    -2.167729\n",
      "      two     0.643310\n",
      "Name: data1, dtype: float64\n",
      "\n",
      "\n",
      "7. Sum by group:\n",
      "         data1     data2\n",
      "key1                    \n",
      "a     2.243182  0.571578\n",
      "b    -1.524420 -1.487892\n",
      "\n",
      "8. Means by group:\n",
      "         data1     data2\n",
      "key1                    \n",
      "a     0.747727  0.190526\n",
      "b    -0.762210 -0.743946\n",
      "\n",
      "9. Means by groupS:\n",
      "              data1     data2\n",
      "key1 key2                    \n",
      "a    one   0.141066  0.026753\n",
      "     two   1.961050  0.518072\n",
      "b    one  -2.167729 -0.823758\n",
      "     two   0.643310 -0.664134\n",
      "\n",
      "10. See the groups:\n",
      "{'a': Int64Index([0, 1, 4], dtype='int64'), 'b': Int64Index([2, 3], dtype='int64')}\n",
      "{('a', 'one'): Int64Index([0, 4], dtype='int64'), ('a', 'two'): Int64Index([1], dtype='int64'), ('b', 'one'): Int64Index([2], dtype='int64'), ('b', 'two'): Int64Index([3], dtype='int64')}\n",
      "{'a': Int64Index([0, 1, 4], dtype='int64'), 'b': Int64Index([2, 3], dtype='int64')}\n",
      "{('a', 'one'): Int64Index([0, 4], dtype='int64'), ('a', 'two'): Int64Index([1], dtype='int64'), ('b', 'one'): Int64Index([2], dtype='int64'), ('b', 'two'): Int64Index([3], dtype='int64')}\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\n",
    "                   'key2' : ['one', 'two', 'one', 'two', 'one'],\n",
    "                   'data1' : np.random.randn(5),\n",
    "                   'data2' : np.random.randn(5)})\n",
    "print('1. DataFrame:\\n{}\\n'.format(df))\n",
    "\n",
    "#Group data 1 column by the key 1:\n",
    "grouped = df['data1'].groupby(df['key1'])\n",
    "grouped2 = df['data1'].groupby([df['key1'], df['key2']])\n",
    "print('2. Group object: {}\\n'.format(grouped))\n",
    "\n",
    "#Operation:\n",
    "print('3. Absolutes by group:\\n{}\\n'.format(grouped.apply(abs)))\n",
    "print('4. Sum by group:\\n{}\\n'.format(grouped.sum()))\n",
    "print('5. Means by group:\\n{}\\n'.format(grouped.mean()))\n",
    "print('6. Means by groupS:\\n{}\\n\\n'.format(grouped2.mean()))\n",
    "\n",
    "#Group all data by the key 1:\n",
    "grouped3 = df.groupby('key1')\n",
    "grouped4 = df.groupby([df['key1'], df['key2']])\n",
    "\n",
    "#Operation:\n",
    "print('7. Sum by group:\\n{}\\n'.format(grouped3.sum()))\n",
    "print('8. Means by group:\\n{}\\n'.format(grouped3.mean()))\n",
    "print('9. Means by groupS:\\n{}\\n'.format(grouped4.mean()))\n",
    "\n",
    "#See the groups:\n",
    "print('10. See the groups:\\n{}\\n{}\\n{}\\n{}'.format(grouped.groups, \n",
    "                                                   grouped2.groups, \n",
    "                                                   grouped3.groups, \n",
    "                                                   grouped4.groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And this is it! All the most interesting and basic features to work with the `pandas` package are the ones that have been presented in this notebook.  Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
